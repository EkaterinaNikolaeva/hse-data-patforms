# Spark с Prefect

## Установка

1. Установите git и python

```
sudo apt install git -y
```

```
sudo apt install python3 -y
```

2. Склонируйте репозиторий командой

```
git clone https://github.com/EkaterinaNikolaeva/hse-data-patforms.git
```

3. Перейдите в директорию проекта

```
cd hse-data-patforms/task-5/
```

4. Для установки необходимых зависимостей выполните

```
python3 main.py prepare
```

Эта команда установит ansible, sshpass, unzip.

5. Для развертывания HDFS и YARN:

* Замените `<pass>` в файле `inventory.ini` на реальный пароль
* При необходимости измените переменные базы данных в `inventory.ini`:
  - `db_user` - пользователь базы данных (по умолчанию: hive)
  - `db_password` - пароль пользователя (по умолчанию: hiveMegaPass)
  - `db_name` - имя базы данных (по умолчанию: metastore)

* Для развертывания hdfs выполните:

```
python3 main.py hdfs
```

Эта команда запустит playbook `run_hdfs.yml`

* Для разертывания YARN выполните:

```
python3 main.py yarn
```

Эта команда запустит playbook `run_yarn.yml`

* Для развертывания Hive выполните:

```
python3 main.py hive
```

Эта команда запустит playbook `run_hive.yml`

* Для использования Spark выполните:

```
python3 main.py spark
```

Эта команда запустит playbook `run_spark.yml` в режиме подготовки окружения (загрузит датасет, подготовит виртуальное окружение, но не запустит обработку данных)

* Для обработки данных через Spark с использованием Beeline выполните:

```
python3 main.py beeline
```

Эта команда запустит playbook `run_spark.yml` с флагом `run_beeline=true`, который:
- Установит необходимые Python-пакеты (pyspark, onetl)
- Создаст базу данных `test` в Hive через Beeline
- Запустит Python-скрипт для создания таблицы `history` и загрузки данных
- Выполнит тестовый запрос через Beeline для проверки корректности загрузки

* Для обработки данных через Spark с использованием Prefect выполните:

```
python3 main.py prefect
```

Эта команда запустит playbook `run_spark.yml` с флагом `run_prefect=true`, который:
- Установит необходимые Python-пакеты из `requirements.txt` (prefect, pyspark, onetl)
- Скопирует и запустит Prefect flow (`prefect_flow.py`)
- Выполнит ETL-процесс с трансформациями данных:
  - **Extract**: чтение данных из HDFS (файл `history.parquet`)
  - **Transform**: преобразование данных (очистка дат, извлечение года/месяца, агрегация по дате)
  - **Load**: запись трансформированных данных в Hive таблицу `test.history` с партиционированием по году

6. Для тестирования функциональности выполните

**Для тестирования MapReduce:**
```
python3 main.py yarn-test
```

Эта команда запустит playbook `yarn_test/test_mapreduce.yml` который:
- Скопирует тестовые файлы из папки `yarn_test/` (mapper.py, reducer.py, file1.txt, file2.txt, file3.txt) на NameNode
- Загрузит тестовые файлы в HDFS
- Запустит MapReduce задачу подсчета слов
- Покажет результаты и сравнит с ожидаемыми

**Для тестирования Hive:**
```
python3 main.py hive-test
```

Эта команда запустит playbook `hive_test/test_beeline.yml` который:
- Проверит доступность HiveServer2 на порту 5433
- Создаст тестовую базу данных и таблицу
- Выполнит тестовые SQL-запросы через Beeline
- Проверит корректность работы Hive
- Очистит тестовые данные

7. Для очистки примененных изменений примените

```
python3 main.py clean
```

Эта команда запустит playbook `cleanup.yml`

**Дополнительные флаги для команды clean:**

```
python3 main.py clean --keep-archives
```
Сохраняет скачанные архивы (Hadoop, Hive) для повторного использования

## Описание режимов работы

### Режим `spark`
Подготавливает окружение для работы со Spark без запуска обработки данных:
- Загружает датасет `capital-weather-data-1995-2024` с Kaggle на jump-узел
- Распаковывает архив и извлекает файл `history.parquet`
- Копирует датасет на NameNode
- Создает виртуальное окружение Python
- Загружает данные в HDFS в директорию `/input/`
- Копирует файлы конфигурации (`requirements.txt`, `prefect_flow.py`)

**Использование:** Подготовка окружения перед запуском обработки данных через `beeline` или `prefect` режимы.

### Режим `beeline`
Обрабатывает данные через Spark с использованием Beeline для работы с Hive:
- Выполняет все шаги из режима `spark` (подготовка окружения)
- Устанавливает Python-пакеты: `pyspark==3.5.6`, `onetl`
- Создает базу данных `test` в Hive через Beeline
- Запускает Python-скрипт (`main.py`), который создает таблицу `history` и загружает данные из HDFS
- Выполняет тестовый SQL-запрос через Beeline для проверки корректности загрузки

**Использование:** Обработка данных без использования Prefect, с прямым взаимодействием через Beeline.

### Режим `prefect`
Обрабатывает данные через Spark с использованием Prefect для оркестрации ETL-процесса:
- Выполняет все шаги из режима `spark` (подготовка окружения)
- Устанавливает Python-пакеты из `requirements.txt`: `prefect>=2.14.0`, `pyspark==3.5.6`, `onetl>=0.9.0`
- Запускает Prefect flow (`prefect_flow.py`), который выполняет полный ETL-процесс:
  - **Extract (Извлечение)**: Читает данные из HDFS (`/input/history.parquet`) используя SparkHDFS и onetl
  - **Transform (Трансформация)**: 
    - Преобразует поле `date` в формат даты (`date_clean`)
    - Фильтрует записи с некорректными датами
    - Извлекает год (`year`) и месяц (`month`) из даты
    - Создает бинарный признак `is_rain` (1 если `rain_mm > 0`, иначе 0)
    - Агрегирует данные по дате, году и месяцу, вычисляя:
      - `rain_max` - максимальное количество осадков за день
      - `rain_min` - минимальное количество осадков за день
      - `rain_avg` - среднее количество осадков за день
      - `rain_events` - количество дождливых дней (сумма `is_rain`)
  - **Load (Загрузка)**: Записывает трансформированные данные в Hive таблицу `test.history` с партиционированием по `year`

**Использование:** Оркестрация ETL-процесса с использованием Prefect для управления задачами и мониторинга выполнения.

## Тестовые файлы

### Тестовые файлы MapReduce

В папке `yarn_test/` включены следующие тестовые файлы для проверки MapReduce:

### Скрипты MapReduce:
- `yarn_test/mapper.py` - скрипт для разбиения текста на слова
- `yarn_test/reducer.py` - скрипт для подсчета частоты слов

### Тестовые данные:
- `yarn_test/file1.txt` - "hello yarn hello hadoop"
- `yarn_test/file2.txt` - "mapreduce python test"  
- `yarn_test/file3.txt` - "yarn cluster working"

### Playbook для тестирования:
- `yarn_test/test_mapreduce.yml` - Ansible playbook для автоматического тестирования

### Ожидаемые результаты:
```
cluster	1
hadoop	2
hello	2
mapreduce	1
python	1
test	1
working	1
yarn	2
```

### Тестовые файлы Hive

В папке `hive_test/` включены следующие тестовые файлы для проверки Hive:

### Playbook для тестирования Hive:
- `hive_test/test_beeline.yml` - Ansible playbook для автоматического тестирования Hive через Beeline

### Процесс тестирования Hive:
1. **Проверка доступности HiveServer2** - ожидание запуска сервера на порту 5433
2. **Создание тестового SQL-скрипта** - генерация скрипта с тестовыми операциями:
   - Создание тестовой базы данных `hive_test`
   - Создание тестовой таблицы `test` с полями `id` и `status`
   - Вставка тестовых данных
   - Выполнение SELECT-запроса для проверки данных
   - Удаление тестовых объектов
3. **Выполнение тестов** - запуск SQL-скрипта через Beeline
4. **Проверка результатов** - валидация успешного выполнения операций
5. **Очистка** - удаление временных файлов

### Ожидаемые результаты тестирования Hive:
- Успешное подключение к HiveServer2
- Создание базы данных и таблицы
- Корректная вставка и выборка данных
- Успешная очистка тестовых объектов

## Проверка работоспособности

1. Для удобного доступа к интерфейсам можно выполнить команду:

```
ssh -L 9870:192.168.1.7:9870 -L 8088:192.168.1.7:8088 -L 19888:192.168.1.7:19888 -L 5433:192.168.1.7:5433 team@<ip>
```
Тогда в браузере можно будет открыть:
* `localhost:9870` — Web-интерфейс NameNode (HDFS)
* `localhost:8088` — Web-интерфейс ResourceManager (YARN)
* `localhost:19888` — Web-интерфейс HistoryServer для MapReduce
* `localhost:5433` — подключение к БД

2. Для подключения к Hive через DBeaver:

**Для подключения используется созданный тунель:**
```
ssh -L 5433:192.168.1.7:5433 team@<ip>
```

**Настройка подключения в DBeaver или похожем инструменте:**
- Database: Apache Hive 
- Host: `localhost`
- Port: `5433`
- Database: `test`
- Username: (оставьте пустым)
- Password: (оставьте пустым)

**Полезные SQL команды для проверки результатов:**
```sql
-- Просмотр партиций таблицы history
SHOW PARTITIONS test.history;

-- Статистика по годам
SELECT year, COUNT(*) as count 
FROM test.history 
GROUP BY year 
ORDER BY year;

-- Выборка данных из конкретной партиции
SELECT * FROM test.history 
WHERE year = 1995 
LIMIT 100;

-- Просмотр структуры таблицы
DESCRIBE test.history;
```

## Процесс развёртывания

### HDFS

Playbook `run_hdfs.yml` выполняет полное развёртывание Hadoop HDFS кластера в несколько этапов:

#### Этап 1: Подготовка пользователей и SSH-доступа
- **Создание пользователя hadoop** - системный пользователь для работы Hadoop на всех узлах
- **Настройка SSH-ключей** - генерация ключей и настройка беспарольного доступа между узлами кластера
- **Обновление /etc/hosts** - конфигурация сетевых имен узлов через шаблон `hosts.j2`

#### Этап 2: Загрузка Hadoop дистрибутива
- **Загрузка архива Hadoop 3.4.0** - выполняется только на jump-узле для экономии трафика
- **Сохранение в директорию /home/team/downloads/** - временное хранение дистрибутива

#### Этап 3: Распространение и установка Hadoop
- **Копирование архива** - распространение Hadoop со jump-узла на все узлы кластера
- **Распаковка дистрибутива** - извлечение в `/home/hadoop/hadoop-3.4.0/`
- **Настройка прав доступа** - рекурсивное назначение владельца hadoop для всех файлов
- **Конфигурация Hadoop** - применение шаблонов конфигурационных файлов:
  - `hadoop-env.sh` - переменные окружения Hadoop
  - `core-site.xml` - основные настройки кластера
  - `hdfs-site.xml` - параметры HDFS
  - `workers` - список DataNode узлов

#### Этап 4: Создание файловых систем
- **Директория NameNode** - создается только на NameNode узлах для метаданных (`/home/hadoop/hadoop_data/namenode`)
- **Директория DataNode** - создается на всех узлах для хранения данных (`/home/hadoop/hadoop_data/datanode`)

#### Этап 5. Инициализация переменных окружения

- Инициализируем переменные окружения HADOOP_HOME, JAVA_HOME

#### Этап 6: Инициализация и запуск HDFS
- **Форматирование HDFS** - инициализация файловой системы (выполняется один раз при первом запуске)
- **Запуск сервисов** - старт демонов NameNode, DataNode и Secondary NameNode

### YARN

#### Этап 1: Копирование конфигурационных файлов

* `mapred-site.xml.j2` задает параметры работы MapReduce на кластере

* `yarn-site.xml.j2` - конфиг-файл YARN

#### Этап 2: Запуск YARN

* Запускаем ResourceManager и NodeManager

* Запускаем MapReduce History Server

### Hive

#### Этап 1: Установка PostgreSQL на DataNode

* **Установка PostgreSQL** - установка сервера PostgreSQL на втором DataNode (dn-01)
* **Создание пользователя hive** - создание пользователя с паролем для доступа к базе данных
* **Создание базы данных metastore** - создание базы данных для хранения метаданных Hive
* **Настройка доступа** - предоставление прав доступа пользователю hive к базе данных metastore
* **Конфигурация аутентификации** - добавление записи в pg_hba.conf для доступа с NameNode
* **Конфигурация PostgreSQL** - настройка прослушивания на адресе DataNode

#### Этап 2: Установка PostgreSQL клиента на NameNode

* **Установка клиента** - установка PostgreSQL клиента для подключения к базе данных

#### Этап 3: Загрузка Hive на jump-узле

* **Загрузка Hive 4.0.0-alpha-2** - скачивание дистрибутива Apache Hive в директорию `/home/team/downloads/`

#### Этап 4: Установка Hive на NameNode

* **Копирование архива** - распространение Hive с jump-узла на NameNode
* **Распаковка архива** - извлечение Hive в домашнюю директорию пользователя hadoop
* **Загрузка JDBC драйвера** - скачивание PostgreSQL JDBC драйвера в lib директорию Hive
* **Конфигурация Hive** - создание файла hive-site.xml с настройками подключения к PostgreSQL

#### Этап 5: Настройка переменных окружения

* **HIVE_HOME** - путь к установленному Hive
* **HIVE_CONF_DIR** - путь к конфигурационным файлам Hive
* **HIVE_AUX_JARS_PATH** - путь к дополнительным JAR файлам
* **PATH** - добавление Hive в переменную PATH
* **Использование blockinfile** - управляемые блоки переменных окружения

#### Этап 6: Настройка HDFS для Hive

* **Создание директорий** - создание `/tmp` и `/user/hive/warehouse` в HDFS
* **Настройка прав доступа** - установка групповых прав записи для директорий

#### Этап 7: Инициализация схемы Hive

* **Создание схемы** - инициализация схемы базы данных для Hive metastore с использованием `initOrUpgradeSchema`

#### Этап 8: Запуск сервисов Hive

* **Hive Metastore** - запуск сервиса метаданных в фоновом режиме
* **HiveServer2** - запуск сервера Hive с отключенной авторизацией в фоновом режиме

### Spark

Playbook `run_spark.yml` выполняет подготовку, загрузку и обработку набора данных в кластере с использованием Apache Spark. Он состоит из трёх основных этапов.

#### Этап 1: Подготовка и загрузка датасета

* **Создание директории для загрузок** — создание каталога `/home/team/downloads`, где будут храниться загружаемые файлы.
* **Загрузка датасета с Kaggle** — скачивание архива `capital-weather-data-1995-2024` по API Kaggle.
* **Распаковка архива** — извлечение содержимого в ту же директорию; извлечённый файл `history.parquet` используется в дальнейшем.

#### Этап 2: Подготовка окружения на NameNode

* **Копирование датасета** — перенос файла `history.parquet` с jump-узла на NameNode в каталог `/home/hadoop/`.
* **Копирование файлов конфигурации** — загрузка файлов `requirements.txt` и `prefect_flow.py` на NameNode.
* **Установка Python virtualenv** — установка пакета `python3-venv` для создания виртуального окружения Python.

#### Этап 3: Обработка данных

* **Загрузка данных в HDFS**
  Создание директории `/input` в HDFS (если не существует) и загрузка файла `history.parquet` в распределённое хранилище.

* **Создание виртуального окружения Python**
  Создание окружения `/home/hadoop/.env`, обновление `pip` и установка необходимых библиотек.

**Режим `beeline`:**
  * Установка библиотек: `pyspark==3.5.6`, `onetl`
  * Создание базы данных `test` в Hive через Beeline
  * Запуск Python-скрипта для создания таблицы `history` и загрузки данных
  * Тестирование подключения к Hive через Beeline

**Режим `prefect`:**
  * Установка библиотек из `requirements.txt`: `prefect>=2.14.0`, `pyspark==3.5.6`, `onetl>=0.9.0`
  * Запуск Prefect flow (`prefect_flow.py`), который выполняет ETL-процесс:
    - **Extract**: Чтение данных из HDFS (`/input/history.parquet`) используя SparkHDFS и onetl
    - **Transform**: Трансформация данных:
      - Преобразование поля `date` в формат даты (`date_clean`)
      - Фильтрация записей с некорректными датами
      - Извлечение года (`year`) и месяца (`month`) из даты
      - Создание бинарного признака `is_rain` (1 если `rain_mm > 0`, иначе 0)
      - Агрегация данных по дате, году и месяцу:
        - `rain_max` - максимальное количество осадков за день
        - `rain_min` - минимальное количество осадков за день
        - `rain_avg` - среднее количество осадков за день
        - `rain_events` - количество дождливых дней (сумма `is_rain`)
    - **Load**: Запись трансформированных данных в Hive таблицу `test.history` с партиционированием по `year`

## Процесс очистки

Playbook `cleanup.yml` выполняет полное удаление Hadoop HDFS кластера, YARN, Hive и всех связанных компонентов в несколько этапов:

**Управление удалением файлов:**
- По умолчанию удаляются все скачанные архивы и данные
- Флаг `--keep-archives` сохраняет архивы Hadoop и Hive для повторного использования

#### Этап 1: Остановка сервисов Hive
- **Остановка HiveServer2** - завершение работы сервера Hive
- **Остановка Hive Metastore** - завершение работы сервиса метаданных

#### Этап 2: Остановка сервисов HDFS
- **Остановка HDFS** - выполнение скрипта `stop-dfs.sh` для корректного завершения работы:
  - NameNode
  - DataNode 
  - Secondary NameNode

#### Этап 3: Принудительное завершение оставшихся процессов
- **Завершение Java-процессов** - принудительное завершение всех Java-процессов пользователя hadoop
- **Завершение всех процессов hadoop** - полная очистка всех процессов, принадлежащих пользователю hadoop

#### Этап 4: Удаление файлов Hive
- **Удаление дистрибутива Hive** - полное удаление директории `/home/hadoop/apache-hive-4.0.0-alpha-2-bin/` со всеми:
  - Исполняемыми файлами
  - Конфигурационными файлами
  - Логами и временными файлами
- **Удаление архива Hive** - удаление скачанного архива Hive с jump-узла (если не используется флаг `--keep-archives`)

#### Этап 5: Удаление PostgreSQL
- **Остановка PostgreSQL** - завершение работы сервера базы данных
- **Удаление пакетов PostgreSQL** - полное удаление PostgreSQL с очисткой конфигурации
- **Удаление данных PostgreSQL** - удаление всех данных и конфигурации базы данных
- **Удаление PostgreSQL клиента** - удаление клиентских пакетов с NameNode

#### Этап 6: Удаление файлов Hadoop
- **Удаление дистрибутива Hadoop** - полное удаление директории `/home/hadoop/hadoop-3.4.0/` со всеми:
  - Исполняемыми файлами
  - Конфигурационными файлами
  - Логами и временными файлами
- **Удаление архива Hadoop** - удаление скачанного архива Hadoop с jump-узла (если не используется флаг `--keep-archives`)
- **Удаление домашней директории** - удаление всей домашней директории пользователя hadoop включая:
  - SSH-ключи
  - Данные HDFS (`hadoop_data`)
  - Временные файлы

#### Этап 7: Удаление системных записей
- **Удаление пользователя hadoop** - полное удаление пользователя c домашней директорией
- **Удаление группы hadoop** - удаление системной группы
