- name: Prepare download dir and fetch dataset on jump node
  hosts: jump_node
  remote_user: team
  gather_facts: no
  tasks:
    - name: Ensure directory exists for download
      file:
        path: /home/team/downloads
        state: directory
        mode: '0755'

    - name: Download dataset
      get_url:
        url: https://www.kaggle.com/api/v1/datasets/download/wafaaelhusseini/capital-weather-data-1995-2024
        dest: /home/team/downloads/input.zip
        mode: '0644'
        timeout: 1800
        force: no

    - name: Unarchive the dataset
      unarchive:
        src: /home/team/downloads/input.zip
        dest: /home/team/downloads
        remote_src: yes
        creates: /home/team/downloads/history.parquet

- name: Preparation and configuration
  hosts: namenodes
  become: yes
  tasks:
    - name: Copy dataset
      copy:
        src: /home/team/downloads/history.parquet
        dest: /home/hadoop/history.parquet
        mode: '0644'
        owner: hadoop
        group: hadoop

    - name: Copy Python script
      copy:
        src: create_history_table.py
        dest: /home/hadoop/main.py
        mode: '0644'
        owner: hadoop
        group: hadoop

    - name: Copy requirements.txt
      copy:
        src: requirements.txt
        dest: /home/hadoop/requirements.txt
        mode: '0644'
        owner: hadoop
        group: hadoop

    - name: Install Python Virtual Environment
      apt:
        name: python3-venv
        state: present

- name: Dataset manipulation
  hosts: namenodes
  become: yes
  become_user: hadoop
  tasks:
    - name: Put dataset to HDFS
      shell: ". ~/.profile && {{ item }}"
      loop:
        - hdfs dfs -test -d /input || hdfs dfs -mkdir -p /input
        - hdfs dfs -put -f /home/hadoop/history.parquet /input/

    - name: Create Python virtual environment
      shell: python3 -m venv /home/hadoop/.env

    - name: Upgrade pip
      shell: . /home/hadoop/.env/bin/activate && pip install -U pip

    - name: Install required Python packages from requirements.txt
      shell: . /home/hadoop/.env/bin/activate && pip install -r /home/hadoop/requirements.txt

    - name: Run beeline to create database 'test'
      shell: |
        . ~/.profile && beeline -u jdbc:hive2://team-1-nn:5433 -e "CREATE DATABASE IF NOT EXISTS test;"
