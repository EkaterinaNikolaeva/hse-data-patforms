# Автоматизированное развертывание кластера hdfs

## Установка

1. Установите git и python

```
sudo apt install git -y
```

```
sudo apt install python3 -y
```

2. Склонируйте репозиторий командой

```
git clone https://github.com/EkaterinaNikolaeva/hse-data-patforms.git
```

3. Перейдите в директорию проекта

```
cd hse-data-patforms/task-1/
```

4. Для установки необходимых зависимостей выполните

```
python3 main.py prepare
```

Эта команда установит ansible, sshpass.

5. Для развертывания HDFS кластера:

* Замените `<pass>` в файле `inventory.ini` на реальный пароль

* Выполните

```
python3 main.py run
```

Эта команда запустит playbook `run_hdfs.yml`

6. Для очистки примененных изменений примените

```
python3 main.py clean
```

Эта команда запустит playbook `cleanup.yml`

## Проверка работоспособности

1. Для удобного доступа к интерфейсу можно выполнить команду:

```
ssh -L 9870:192.168.1.7:9870 team@<ip>
```

тогда в браузере можно будет открыть вкладку по адресу `localhost:9870`

2. Для проверки работоспобности на хостах системы можно выполнить команду

```
sudo -u hadoop jps
```

ожидаемый вывод вида

```
22697 Jps
22524 DataNode
```

## Процесс развёртывания

Playbook `run_hdfs.yml` выполняет полное развёртывание Hadoop HDFS кластера в несколько этапов:

#### Этап 1: Подготовка пользователей и SSH-доступа
- **Создание пользователя hadoop** - системный пользователь для работы Hadoop на всех узлах
- **Настройка SSH-ключей** - генерация ключей и настройка беспарольного доступа между узлами кластера
- **Обновление /etc/hosts** - конфигурация сетевых имен узлов через шаблон `hosts.j2`

#### Этап 2: Загрузка Hadoop дистрибутива
- **Загрузка архива Hadoop 3.4.0** - выполняется только на jump-узле для экономии трафика
- **Сохранение в директорию /home/team/downloads/** - временное хранение дистрибутива

#### Этап 3: Распространение и установка Hadoop
- **Копирование архива** - распространение Hadoop со jump-узла на все узлы кластера
- **Распаковка дистрибутива** - извлечение в `/home/hadoop/hadoop-3.4.0/`
- **Настройка прав доступа** - рекурсивное назначение владельца hadoop для всех файлов
- **Конфигурация Hadoop** - применение шаблонов конфигурационных файлов:
  - `hadoop-env.sh` - переменные окружения Hadoop
  - `core-site.xml` - основные настройки кластера
  - `hdfs-site.xml` - параметры HDFS
  - `workers` - список DataNode узлов

#### Этап 4: Создание файловых систем
- **Директория NameNode** - создается только на NameNode узлах для метаданных (`/home/hadoop/hadoop_data/namenode`)
- **Директория DataNode** - создается на всех узлах для хранения данных (`/home/hadoop/hadoop_data/datanode`)

#### Этап 5: Инициализация и запуск HDFS
- **Форматирование HDFS** - инициализация файловой системы (выполняется один раз при первом запуске)
- **Запуск сервисов** - старт демонов NameNode, DataNode и Secondary NameNode

### Процесс очистки

Playbook `cleanup.yml` выполняет полное удаление Hadoop HDFS кластера и всех связанных компонентов в несколько этапов:

#### Этап 1: Остановка сервисов HDFS
- **Остановка HDFS** - выполнение скрипта `stop-dfs.sh` для корректного завершения работы:
  - NameNode
  - DataNode 
  - Secondary NameNode

#### Этап 2: Принудительное завершение оставшихся процессов
- **Завершение Java-процессов** - принудительное завершение всех Java-процессов пользователя hadoop
- **Завершение всех процессов hadoop** - полная очистка всех процессов, принадлежащих пользователю hadoop

#### Этап 3: Удаление файлов Hadoop
- **Удаление дистрибутива Hadoop** - полное удаление директории `/home/hadoop/hadoop-3.4.0/` со всеми:
  - Исполняемыми файлами
  - Конфигурационными файлами
  - Логами и временными файлами
- **Удаление домашней директории** - удаление всей домашней директории пользователя hadoop включая:
  - SSH-ключи
  - Данные HDFS (`hadoop_data`)
  - Временные файлы

#### Этап 4: Удаление системных записей
- **Удаление пользователя hadoop** - полное удаление пользователя c домашней директорией
- **Удаление группы hadoop** - удаление системной группы
